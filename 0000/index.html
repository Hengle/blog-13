<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>The Hadamard Product and Its Properties</title>

    <!-- Bootstrap -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">
    <link href="../css/theme.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- MathJax -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
    </script>    
    <script type="text/javascript"
            src="../MathJax/MathJax.js?config=TeX-AMS_HTML-full">
    </script>
</head>
<body>
<div class="container">
    \(
    \newcommand{\ve}[1]{\mathbf{#1}}
    \newcommand{\diag}{\mathrm{diag}}
    \newcommand{\Real}{\mathbb{R}}
    \newcommand{\tr}{\mathbb{tr}}
    \)

    <div class="page-header"><h1>The Hadarmard Product and Its Properties</h1></div>

    <p>
        I encountered the Hadamard product when I took
        <a href="http://www.cs.cornell.edu/courses/cs4780/2017sp/index.html">CS 4780: Machine Learning</a>
        in the Spring of 2017.  There was a problem in a homework assignment that relies on
        the Schur product theorem (to be proven later in this blog post).
        The theorem is non-trivial that I could not prove it after spending some time,
        so I raised the white flag and searched the Internet for some hints.
        It turned out this was a significant theorem that warrented some study after all.
    </p>

    <p>
        What you see in this blog post is mostly taken from a note by
        <a href="http://buzzard.ups.edu/courses/2007spring/projects/million-paper.pdf">Elizabeth Million</a>,
        the Wikipedia article on the
        <a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)">Hadamard product</a>,
        the one on the <a href="https://en.wikipedia.org/wiki/Schur_product_theorem">Schur product theorem</a>,
        <a href="http://www.math.ucsd.edu/~njw/Teaching/Math271C/Lecture_03.pdf">this note on positive semidefinite
        matrices</a>, and the book by <a href="https://www.amazon.com/Matrix-Analysis-Roger-Horn/dp/0521386322">
        Horn and Johnson</a>.
    </p>

    <h2>Definition and Basic Properties</h2>

    <p>
        Given two matrices $A$ and $B$ of the same size, the <b>Hadamard product</b> of $A$ and $B$,
        denoted $A \circ B$ is given by:
        \begin{align*}
        [A \circ B]_{ij} = [A]_{ij} [B]_{ij}.
        \end{align*}
        Namely, we multiply the matrices element-wise.
        The following properties of the Hadamard product are trivial to show:
    </p>
    <ol>
        <li>
            $A \circ B = B \circ A$
        </li>
        <li>
            The identity under the Hadamard product is the matrix whose entries are all $1$s.
        </li>
        <li>
            Let $A$ be a matrice none of whose entries is zero.  Then, the inverse of
            $A$ (denoted by $\hat{A}$) under the Hadamard product exists and is given by
            $[\hat{A}]_{ij} = ([A]_{ij})^{-1}$.
        </li>
        <li>
            The Hadamard product is linear.  That is:
            <ul>
                <li>$C \circ (A + B) = C \circ A + C \circ B$</li>
                <li>$\alpha(A \circ B) = (\alpha A) \circ B = A \circ (\alpha B)$ for any scalar $\alpha$.</li>
            </ul>            
        </li>        
    </ol>

    <p>
        The note by Million mentioned casually that
        "$A \circ B = AB$ if and only if $A$ and $B$ are diagonal."  This is patently false because:
        \begin{align*}
        \begin{bmatrix}
        0 & 1 \\
        0 & 0
        \end{bmatrix}
        \circ
        \begin{bmatrix}
        1 & 0 \\
        0 & 0
        \end{bmatrix}
        =
        \begin{bmatrix}
        0 & 0 \\
        0 & 0
        \end{bmatrix}
        =
        \begin{bmatrix}
        0 & 1 \\
        0 & 0
        \end{bmatrix}
        \begin{bmatrix}
        1 & 0 \\
        0 & 0
        \end{bmatrix}.
        \end{align*}
    </p>

    <h2>The Trace Formula</h2>

    <p>
        For any vector $\ve{x} = (x_1, x_2, \dotsc, x_n)^T$, let $D_\ve{x}$ denotes the diagonal matrix whose
        diagonal entries are the entries of $\ve{x}$.  Namely,
        \begin{align*}
        D_\ve{x} = \begin{bmatrix}
        x_1 & & & \\
        & x_2 & & \\
        & & \ddots & \\
        & & & x_n
        \end{bmatrix}.
        \end{align*}
    </p>

    <p>
        <b>Theorem 1 (Trace Formula).</b> For any $A, B \in \Real^{m\times n}$ and $\ve{x} \in \Real^m$ and
        $\ve{y} \in \Real^n$, we have that
        $$\ve{x}^T (A \circ B) \ve{y} = \tr(D_\ve{x} A D_\ve{y} B^T).$$
    </p>

    <p>
        <i>Proof.</i>
        \begin{align*}
        \ve{x}^T (A \circ B) \ve{y}
        &= \sum_{i=1}^m \sum_{j=1}^n [\ve{x}]_i [A \circ B]_{ij} [\ve{y}]_j
        = \sum_{i=1}^m \sum_{j=1}^n [\ve{x}]_i [A]_{ij}[B]_{ij} [\ve{y}]_j
        = \sum_{i=1}^m \sum_{j=1}^n [D_\ve{x} A]_{ij}[B D_\ve{y} ]_{ij} \\
        &= \sum_{i=1}^m \sum_{j=1}^n [D_\ve{x} A]_{ij}[(B D_\ve{y})^T]_{ji}
        = \sum_{i=1}^m \sum_{j=1}^n [D_\ve{x} A]_{ij}[D_\ve{y} B^T]_{ji}
        = \sum_{i=1}^m [D_\ve{x} A D_\ve{y} B^T]_{ii} \\
        &= \tr(D_\ve{x} A D_\ve{y} B^T).
        \tag*{$\Box$}
        \end{align*}
    </p>

    <h2>The Schur Product Theorem</h2>

    <p>
        The Schur product theorem connects the Hadamard product of matrices with positive semidefiniteness.
        Recall that a real symmetric matrix $A \in \Real^{n\times n}$ is
        <i>positive semidefinite</i> if $\ve{x}^T A \ve{x} \geq 0$ for all $\ve{x} \in \Real^n$.
        Also, recall that the following properties are equivalent:
        <ul>
            <li>$A$ is positive semidifinite.</li>
            <li>All the eigenvalues of $A$ are non-negative.</li>
            <li>There exists $B$ such that $A = B^T B$.</li>
        </ul>
    </p>
    <p>
        The matrix $A$ is <i>positive definite</i> if $\ve{x}^T A \ve{x} > 0$ for all $\ve{x} \neq \ve{0}$.
        (In other words, if $\ve{x}^T A \ve{x} = 0$, then $\ve{x} = \ve{0}$.)
        Similar to the properties of positive semidefinite matrices, the following properties are equivalent:
        <ul>
            <li>$A$ is positive definite.</li>
            <li>All the eigenvalues of $A$ are positive.</li>
            <li>There exists a non-singular $B$ such that $A = B^T B$.</li>
        </ul>
    </p>

    <p>
        <b>Lemma 1.</b> If $A \in \Real^{n\times n}$ is a positive semidefinite matrix of rank $k$, then $A$ may be written in the form
        $A = \ve{v}_1 \ve{v}_1^T + \ve{v}_2 \ve{v}_2^T + \dotsb + \ve{v}_k \ve{v}_k^T$ where $\ve{u}_i \in \Real^n$.
    </p>
    <p>
        <i>Proof.</i> Using the eigendecomposition of $A$, we have that $A = U \Lambda U^T$.  Let $\ve{u}_i$ denote
        the $i$th column of $U$, and let $\lambda_i$ denote the $i$th eigenvalue of $A$.
        Then, $U \Lambda U^T = \sum_{i=1}^k \lambda_i \ve{u}_i \ve{u}_i^T$.
        Since $\lambda_i > 0$, we have that we may write:
        $\sum_{i=1}^k \lambda_i \ve{u}_i \ve{u}_i^T = \sum_{i=1}^k (\sqrt{\lambda_i}  \ve{u}_i) (\sqrt{\lambda_i} \ve{u}_i)^T$.
        Set $\ve{v}_i = \sqrt{\lambda_i} \ve{u}^T$, and we are done. $\Box$
    </p>

    <p>
        <b>Lemma 2.</b>  For any vector $\ve{v}, \ve{w} \in \Real^n$, we have that
        $\ve{v} \ve{v}^T \circ \ve{w}\ve{w}^T = (\ve{v} \circ \ve{w})(\ve{v} \circ \ve{w})^T$
    </p>
    <p>
        <i>Proof.</i>
        \begin{align*}
        [\ve{v} \ve{v}^T \circ \ve{w}\ve{w}^T]_{ij}
        &= [\ve{v} \ve{v}^T]_{ij} [\ve{w}\ve{w}^T]_{ij}
        = [\ve{v}]_i [\ve{v}]_{j} [\ve{w}]_{i} [\ve{w}]_j
        = ([\ve{v}]_i [\ve{w}]_{i}) ([\ve{v}]_{j}  [\ve{w}]_j)
        = [\ve{v} \circ \ve{w}]_i [\ve{v} \circ \ve{w}]_j
        = [(\ve{v} \circ \ve{w})(\ve{v} \circ \ve{w})^T]_{ij}.
        \end{align*}
        As a result, $\ve{v} \ve{v}^T \circ \ve{w}\ve{w}^T = (\ve{v} \circ \ve{w})(\ve{v} \circ \ve{w})^T$. $\Box$
    </p>

    <p><b>Theorem 2 (Schur product theorem).</b> If $A$ and $B$ are positive semidefinite matrices, then
    so is $A \circ B$.  Moreover, if both $A$ and $B$ are positive definite, then so is $A \circ B$.</p>

    <p>
        <i>Proof.</i> We show the theorem is true for the case of positive semidefinite matrices first.
        Using Lemma 1, we may write $A = \sum_{i=1}^k \ve{v}_i \ve{v}_i^T$ and $B = \sum_{j=1}^l \ve{w}_j \ve{w}_j^T$
        So,
        \begin{align*}
        A \circ B
        &= \bigg( \sum_{i=1}^k \ve{v}_i \ve{v}_i^T \bigg) \circ \bigg( \sum_{j=1}^l \ve{w}_j \ve{w}_j^T \bigg)
        = \sum_{i=1}^k \sum_{j=1}^l \ve{v}_i \ve{v}_i^T \circ \ve{w}_j \ve{w}_j^T.
        \end{align*}
        Using Lemma 2, the above expression becomes:
        \begin{align*}
        A \circ B
        &= \sum_{i=1}^k \sum_{j=1}^l (\ve{v}_i \circ \ve{w}_j) (\ve{v}_i \circ \ve{w}_j)^T.
        \end{align*}
        Since $(\ve{v}_i \circ \ve{w}_j) (\ve{v}_i \circ \ve{w}_j)^T$ is positive semidefinite, we have that we can
        write $A \circ B$ as a sum of positive semidefinite matrices.  Thus, $A \circ B$ is positive semidefinite.
    </p>
    <p>
        Next, we show the result for positive definite matrices.  If $A$ and $B$ are both positive definite, then
        $k = l = n$.  Moreover, the set $\{ \ve{v}_i \}$ and $\{ \ve{w}_j \}$ are orthonormal bases of $\Real^n$.
        Let $\ve{x}$ be a vector such that $\ve{x}^T (A \circ B) \ve{x} = 0$.  We have that:
        \begin{align*}
        0
        = \ve{x}^T (A \circ B) \ve{x}
        = \ve{x}^T \bigg( \sum_{i=1}^n \sum_{j=1}^n (\ve{v}_i \circ \ve{w}_j) (\ve{v}_i \circ \ve{w}_j)^T \bigg) \ve{x}
        = \sum_{i=1}^n \sum_{j=1}^n \ve{x}^T (\ve{v}_i \circ \ve{w}_j) (\ve{v}_i \circ \ve{w}_j)^T \ve{x}
        = \sum_{i=1}^n \sum_{j=1}^n ((\ve{v}_i \circ \ve{w}_j)^T \ve{x})^2.
        \end{align*}
        This means that $(\ve{v}_i \circ \ve{w}_j)^T \ve{x} = 0$ for all combinations of $i$ and $j$.  Now,
        \begin{align*}
        0
        = (\ve{v}_i \circ \ve{w}_j)^T \ve{x}
        = \sum_{k=1}^n [\ve{v}_i \circ \ve{w}_j]_k [\ve{x}]_k
        = \sum_{k=1}^n [\ve{v}_i]_k [\ve{w}_j]_k [\ve{x}]_k
        = \sum_{k=1}^n [\ve{v}_i]_k [\ve{x}]_k [\ve{w}_j]_k
        = \sum_{k=1}^n [\ve{v}_i \circ \ve{x}]_k [\ve{w}_j]_k
        = (\ve{v}_i \circ \ve{x})^T \ve{w}_j
        \end{align*}
        This means that, $\ve{v}_i \circ \ve{x}$ is perpendicular to every vector in the set $\{ \ve{w}_j \}$,
        which is a basis of $\Real^n$.  This means that $\ve{v}_i \circ \ve{x} = \ve{0}$ for all $i$.  Since
        $\{ \ve{v}_i \}$ is a basis of $\Real^n$, it must be the case that there exists a permutation $\pi$ such
        that $[\ve{v}_i]_{\pi(i)} \neq 0$ for all $i$.  (Otherwise, the determinant of matrices constructed
        from the column vectors $\ve{v}_i$ would be zero.)  This implies that, for all $i$,
        $[\ve{v}_i \circ \ve{x}]_{\pi(i)} = 0$, and so $[\ve{x}]_{\pi(i)} = 0$.
        As a result, $\ve{x} = \ve{0}$.  We can now conclude that $A \circ B$ is positive definite. $\Box$
    </p>

    <div class="page-header"></div>
    <p>Last modified: 2017/05/08</p>
</div>

</body>
</html>